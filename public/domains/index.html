<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>AI Agent CMM -- Domain Deep-Dive</title>
<style>
*, *::before, *::after { box-sizing: border-box; margin: 0; padding: 0; }

:root {
  --color-bg: #f8f9fa;
  --color-surface: #ffffff;
  --color-border: #dee2e6;
  --color-text: #212529;
  --color-text-muted: #6c757d;
  --color-primary: #2563eb;
  --color-primary-light: #dbeafe;
  --color-red: #dc2626;
  --color-red-light: #fef2f2;
  --color-green: #16a34a;
  --color-green-light: #f0fdf4;
  --radius: 8px;
  --shadow: 0 1px 3px rgba(0,0,0,0.1), 0 1px 2px rgba(0,0,0,0.06);
  --shadow-lg: 0 4px 12px rgba(0,0,0,0.1), 0 2px 4px rgba(0,0,0,0.06);
  --color-l0: #94a3b8;
  --color-l1: #f97316;
  --color-l2: #eab308;
  --color-l3: #3b82f6;
  --color-l4: #8b5cf6;
  --color-l5: #16a34a;
  --color-domain-1: #3b82f6;
  --color-domain-2: #8b5cf6;
  --color-domain-3: #06b6d4;
  --color-domain-4: #10b981;
  --color-domain-5: #f59e0b;
  --color-domain-6: #ef4444;
  --color-domain-7: #ec4899;
  --color-domain-8: #6366f1;
  --color-domain-9: #14b8a6;
}

body {
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
  background: var(--color-bg);
  color: var(--color-text);
  line-height: 1.6;
}

.container {
  max-width: 960px;
  margin: 0 auto;
  padding: 24px 20px;
}

.header {
  text-align: center;
  padding: 16px 0 8px;
  border-bottom: 1px solid var(--color-border);
  margin-bottom: 24px;
}
.header h1 {
  font-size: 1.1rem;
  font-weight: 600;
  color: var(--color-text-muted);
  letter-spacing: 0.02em;
}

/* --- Domain nav --- */
.domain-nav {
  display: flex;
  flex-wrap: wrap;
  gap: 6px;
  margin-bottom: 24px;
  justify-content: center;
}
.domain-nav-btn {
  padding: 6px 14px;
  border: 2px solid var(--color-border);
  border-radius: 20px;
  background: var(--color-surface);
  font-size: 0.8rem;
  font-weight: 600;
  cursor: pointer;
  transition: all 0.15s ease;
  color: var(--color-text);
}
.domain-nav-btn:hover {
  border-color: var(--color-primary);
  color: var(--color-primary);
}
.domain-nav-btn.active {
  background: var(--color-primary);
  border-color: var(--color-primary);
  color: #fff;
}

/* --- Domain content --- */
.domain-page { display: none; }
.domain-page.active { display: block; }

.domain-title-card {
  background: var(--color-surface);
  border-radius: var(--radius);
  box-shadow: var(--shadow-lg);
  padding: 28px 32px;
  margin-bottom: 20px;
  border-top: 4px solid;
}
.domain-title-card .domain-num {
  font-size: 0.78rem;
  font-weight: 600;
  text-transform: uppercase;
  letter-spacing: 0.04em;
  color: var(--color-text-muted);
  margin-bottom: 4px;
}
.domain-title-card h2 {
  font-size: 1.4rem;
  margin-bottom: 8px;
}
.domain-title-card p {
  font-size: 0.92rem;
  color: var(--color-text-muted);
}

/* --- Level cards --- */
.level-card {
  background: var(--color-surface);
  border-radius: var(--radius);
  box-shadow: var(--shadow);
  padding: 24px 28px;
  margin-bottom: 12px;
  border-left: 4px solid;
}
.level-card-header {
  display: flex;
  align-items: center;
  gap: 12px;
  margin-bottom: 14px;
  cursor: pointer;
}
.level-badge {
  display: flex;
  align-items: center;
  justify-content: center;
  width: 36px;
  height: 36px;
  border-radius: 50%;
  font-size: 1rem;
  font-weight: 700;
  color: #fff;
  flex-shrink: 0;
}
.level-card-header h3 {
  font-size: 1.05rem;
  flex: 1;
}
.level-toggle {
  font-size: 0.8rem;
  color: var(--color-text-muted);
  transition: transform 0.2s ease;
}
.level-card.collapsed .level-toggle { transform: rotate(-90deg); }
.level-card-body {
  display: block;
}
.level-card.collapsed .level-card-body { display: none; }

.detail-section {
  margin-bottom: 14px;
}
.detail-section h4 {
  font-size: 0.78rem;
  text-transform: uppercase;
  letter-spacing: 0.04em;
  color: var(--color-text-muted);
  margin-bottom: 6px;
}
.detail-section ul {
  padding-left: 20px;
  font-size: 0.88rem;
}
.detail-section li {
  margin-bottom: 4px;
  line-height: 1.5;
}
.anti-pattern-box {
  padding: 12px 16px;
  background: var(--color-red-light);
  border-radius: 6px;
  font-size: 0.85rem;
  color: #991b1b;
}
.artifact-item {
  padding: 4px 0;
  font-size: 0.88rem;
}

/* --- Prev/next nav --- */
.page-nav {
  display: flex;
  justify-content: space-between;
  padding: 12px 0 24px;
}
.btn {
  display: inline-flex;
  align-items: center;
  gap: 8px;
  padding: 10px 22px;
  border: none;
  border-radius: var(--radius);
  font-size: 0.9rem;
  font-weight: 600;
  cursor: pointer;
  transition: all 0.2s ease;
  text-decoration: none;
}
.btn-primary { background: var(--color-primary); color: #fff; }
.btn-primary:hover { background: #1d4ed8; }
.btn-secondary { background: var(--color-surface); color: var(--color-text); border: 1px solid var(--color-border); }
.btn-secondary:hover { background: var(--color-bg); }

/* --- Responsive --- */
@media (max-width: 640px) {
  .container { padding: 16px 12px; }
  .domain-title-card { padding: 20px 16px; }
  .level-card { padding: 16px; }
  .domain-nav { gap: 4px; }
  .domain-nav-btn { padding: 5px 10px; font-size: 0.75rem; }
}

/* --- Print --- */
@media print {
  body { background: #fff; }
  .domain-nav, .page-nav, .header { display: none !important; }
  .domain-page { display: block !important; break-before: page; }
  .domain-page:first-child { break-before: auto; }
  .level-card { box-shadow: none; border: 1px solid #ddd; break-inside: avoid; }
  .level-card.collapsed .level-card-body { display: block !important; }
  .domain-title-card { box-shadow: none; border: 1px solid #ddd; break-inside: avoid; }
}
</style>
</head>
<body>

<div class="container">
  <div class="header">
    <h1>AI Agent CMM -- Domain Deep-Dive</h1>
  </div>

  <div class="domain-nav" id="domain-nav"></div>
  <div id="domains-container"></div>
</div>

<script>
var LEVEL_NAMES = ['Unaware','Exploratory','Defined','Managed','Optimized','Adaptive'];
var LEVEL_COLORS = ['var(--color-l0)','var(--color-l1)','var(--color-l2)','var(--color-l3)','var(--color-l4)','var(--color-l5)'];
var DOMAIN_COLORS = [
  'var(--color-domain-1)','var(--color-domain-2)','var(--color-domain-3)',
  'var(--color-domain-4)','var(--color-domain-5)','var(--color-domain-6)',
  'var(--color-domain-7)','var(--color-domain-8)','var(--color-domain-9)'
];

var DOMAINS = [
  {
    name: 'Dev Workflow',
    description: 'How agents are used during local development -- IDE integration, dev environments, CLI tooling, and configuration management.',
    levels: [
      {
        practices: ['None.'],
        artifacts: ['None.'],
        antiPatterns: 'Developers use personal API keys for commercial AI services. No awareness of data exposure risks.'
      },
      {
        practices: ['Some developers use IDE-integrated agents (Copilot, Claude Code) for code completion and generation.'],
        artifacts: ['None required; informal sharing of tips may exist.'],
        antiPatterns: 'No distinction between approved and unapproved tools. Proprietary code is sent to unknown third-party endpoints.'
      },
      {
        practices: [
          'Approved agent tools list is published and maintained.',
          'Development environment definitions include agent tooling and configuration (extensions, CLI tools, project-level config files).',
          'Developers use only org-approved API endpoints (e.g., cloud-hosted model APIs or an org-managed proxy).',
          'Project-level agent instructions exist in repositories (e.g., CLAUDE.md, .github/copilot-instructions.md, or equivalent).'
        ],
        artifacts: [
          'Approved tools list (living document or wiki page).',
          'Development environment configuration with agent tooling pre-installed.',
          'Repository-level agent instructions file.'
        ],
        antiPatterns: 'Approving tools but not configuring them; developers still use personal accounts. Dev environments exist but do not include agent tooling.'
      },
      {
        practices: [
          'Agent usage telemetry is collected (opt-in or org-managed): number of completions, acceptances, languages, repos.',
          'Development environment images are built and published through your artifact registry with agent tooling baked in.',
          'Standardized agent instruction templates are maintained per project type (service, library, IaC module).',
          'Context management is intentional: agents are given curated docs, not raw repo dumps.'
        ],
        artifacts: [
          'Telemetry dashboard (e.g., cloud monitoring service, Datadog, Grafana, or internal tool).',
          'Registry-hosted dev environment images with version tags.',
          'Agent instruction template library.'
        ],
        antiPatterns: 'Collecting telemetry but never reviewing it. Dev environment images are stale and lag behind tooling updates.'
      },
      {
        practices: [
          'Agents are embedded in developer workflows end-to-end: scaffold, implement, test, document.',
          'Agent-generated code is automatically labeled in commits and PRs (trailers, labels, or metadata).',
          'IDE agent configurations are version-controlled and deployed via dev environment features or setup scripts.',
          'Cross-repo context is available to agents through indexed documentation or retrieval-augmented generation (RAG).'
        ],
        artifacts: [
          'Commit trailer convention (e.g., Co-Authored-By: Claude <noreply@anthropic.com>).',
          'Dev environment feature or configuration package for agent setup.',
          'Documentation index or RAG pipeline.'
        ],
        antiPatterns: 'Agents have unrestricted filesystem or network access in dev environments. No differentiation between agent-assisted and human-written code.'
      },
      {
        practices: [
          'Agent configurations are tuned per-team and per-project based on measured quality and velocity outcomes.',
          'New agent capabilities (tool use, multi-file edits, autonomous tasks) are evaluated through structured pilots before broad rollout.',
          'Feedback loops exist: developers rate agent suggestions, and those ratings inform prompt and context improvements.'
        ],
        artifacts: [
          'Agent configuration tuning log or changelog.',
          'Pilot evaluation reports.',
          'Developer satisfaction survey results (quarterly or per-pilot).'
        ],
        antiPatterns: 'Adopting every new agent feature on day one without evaluation. Ignoring negative developer feedback about agent quality.'
      }
    ]
  },
  {
    name: 'PR and Code Review',
    description: 'How agent-generated code is disclosed, reviewed, labeled, and merged. Review rigor, quality tracking, and risk-adjusted processes.',
    levels: [
      {
        practices: ['None.'],
        artifacts: ['None.'],
        antiPatterns: 'Agent-generated code is committed without any review. Reviewers do not know which code was agent-generated.'
      },
      {
        practices: ['Some developers disclose agent usage in PR descriptions informally.'],
        artifacts: ['None required.'],
        antiPatterns: 'Agent-generated PRs are merged without human review. Large agent-generated diffs are rubber-stamped.'
      },
      {
        practices: [
          'PR template includes a checkbox or field for declaring agent usage.',
          'Agent-generated PRs require at least one human reviewer.',
          'PR checklist includes agent-specific items.'
        ],
        artifacts: [
          'PR template with agent disclosure field.',
          'Branch protection or merge request approval rules enforcing at least one approval.'
        ],
        antiPatterns: 'Checklist exists but reviewers skip agent-specific items. No difference in review rigor for agent-generated code.'
      },
      {
        practices: [
          'Agent-generated PRs are automatically labeled (via CI automation or bot).',
          'Review metrics are tracked separately for agent-generated vs. human-written PRs: time-to-merge, defect rate, rework rate.',
          'Reviewers receive guidance on what to look for in agent-generated code (over-abstraction, hallucinated imports, incorrect error handling).'
        ],
        artifacts: [
          'CI automation or bot that auto-labels agent PRs.',
          'Review guidance document.',
          'Dashboard splitting PR metrics by agent vs. human origin.'
        ],
        antiPatterns: 'Auto-labeling is noisy or inaccurate, causing label fatigue. Metrics are collected but not acted on.'
      },
      {
        practices: [
          'Automated pre-review checks run on agent-generated PRs: lint, type-check, test coverage delta, dependency audit, secrets scan.',
          'High-risk changes from agents require additional reviewer or team lead approval.',
          'Agent-generated PRs include structured context: the prompt used, files read, and changes made.'
        ],
        artifacts: [
          'CI gate configuration for agent PRs.',
          'Code ownership rules for high-risk paths (e.g., CODEOWNERS or equivalent).',
          'Structured PR description template showing agent context.'
        ],
        antiPatterns: 'Over-reliance on automated checks; humans stop reading diffs. Code ownership rules are outdated and do not reflect actual ownership.'
      },
      {
        practices: [
          'Review rigor is dynamically adjusted based on change risk score (file sensitivity, blast radius, historical defect rate).',
          'Agent-generated PRs that pass all automated checks and touch only low-risk files may follow an expedited review path.',
          'Defect data feeds back into agent prompts and guardrails to reduce recurring issues.'
        ],
        artifacts: [
          'Risk scoring model or heuristic for PRs.',
          'Expedited review policy with clear boundaries.',
          'Feedback loop documentation showing prompt/guardrail changes driven by defect data.'
        ],
        antiPatterns: 'Expedited review becomes the default, bypassing human judgment. Risk scoring is never recalibrated.'
      }
    ]
  },
  {
    name: 'CI/CD',
    description: 'How agents participate in build, test, and deploy pipelines. Isolation, auditability, pipeline standards, and autonomous maintenance.',
    levels: [
      {
        practices: ['None.'],
        artifacts: ['None.'],
        antiPatterns: 'No CI/CD exists, or agents have no interaction with pipelines.'
      },
      {
        practices: ['Developers occasionally ask agents to help write or debug CI/CD pipelines.'],
        artifacts: ['None required.'],
        antiPatterns: 'Agent-generated pipeline files are committed without testing. Agents suggest using security-sensitive triggers or configurations without understanding the risk.'
      },
      {
        practices: [
          'Agents are not permitted to modify CI/CD pipeline definitions without human review (enforced via code ownership rules on pipeline config directories).',
          'A library of approved, reusable pipeline components exists and agents are instructed to use them.',
          'Pipeline definitions follow a standard structure documented in agent instructions or a contributing guide.'
        ],
        artifacts: [
          'Code ownership rules for pipeline configuration files.',
          'Reusable pipeline component library (in a shared repo or artifact registry).',
          'Pipeline authoring guide.'
        ],
        antiPatterns: 'Agents freely create new pipelines without review. No reusable components; every repo has bespoke pipeline logic.'
      },
      {
        practices: [
          'Agents run as CI steps for specific tasks: generating test cases, running linters with auto-fix, updating documentation.',
          'Agent CI steps run in isolated containers with no access to deployment credentials.',
          'Agent CI step outputs are logged and auditable.'
        ],
        artifacts: [
          'Agent CI step definitions (containerized, least-privilege).',
          'Audit log configuration (CI logs retained, forwarded to your monitoring/SIEM system).',
          'Container images for agent CI steps published in your artifact registry.'
        ],
        antiPatterns: 'Agent CI steps have access to production secrets. Agent CI outputs are not logged. Agent containers are pulled from public registries without verification.'
      },
      {
        practices: [
          'Agents can propose pipeline improvements (caching, parallelism, dependency updates) as PRs, subject to human review.',
          'Agent-in-CI usage is measured: cost per run, time added to pipeline, value delivered (bugs caught, tests generated).',
          'Deployment pipelines include agent-aware gates: if a PR is agent-generated and touches high-risk paths, additional checks run.'
        ],
        artifacts: [
          'Cost and value dashboard for agent CI steps.',
          'Agent-aware gate configuration.',
          'Pipeline improvement PR history.'
        ],
        antiPatterns: 'Agent CI steps add significant time to pipelines without measurable value. Agents propose pipeline changes that break other repos using shared workflows.'
      },
      {
        practices: [
          'Agents autonomously maintain CI health: update pinned dependency versions, rotate test fixtures, fix flaky tests (with human approval on the resulting PR).',
          'Pipeline configurations are generated from higher-level intent files, with agents handling the translation.',
          'Agent CI contributions are measured against the same quality bar as human contributions.'
        ],
        artifacts: [
          'Intent-based pipeline definition format (if applicable).',
          'Autonomous maintenance policy with approval requirements.',
          'Quality parity report (agent vs. human CI contributions).'
        ],
        antiPatterns: 'Agents make autonomous changes to production pipelines without approval. "Intent-based" pipelines become an abstraction that nobody understands.'
      }
    ]
  },
  {
    name: 'Infrastructure as Code',
    description: 'How agents generate, modify, and validate IaC (Terraform, OpenTofu, Pulumi, CloudFormation). Module registries, policy-as-code, and drift detection.',
    levels: [
      {
        practices: ['None.'],
        artifacts: ['None.'],
        antiPatterns: 'Infrastructure is manually provisioned. No IaC exists.'
      },
      {
        practices: ['Developers use agents to generate IaC snippets or modules in their IDE.'],
        artifacts: ['None required.'],
        antiPatterns: 'Agent-generated IaC is applied without plan review. Agents generate resources with overly permissive IAM/RBAC policies (e.g., "Action": "*").'
      },
      {
        practices: [
          'Agents are instructed (via project-level config) to follow org IaC conventions: module structure, naming, tagging, backend configuration.',
          'Agent-generated IaC must pass validation and a linter (tflint, checkov, tfsec) before merge.',
          'Agents are prohibited from generating identity/access policies with wildcard actions or resources unless explicitly approved.',
          'Regulated environment modules are separated from standard modules, with distinct variable files and backend configurations.'
        ],
        artifacts: [
          'IaC style guide.',
          'Linter configuration files (checked into repos).',
          'Agent instructions with IaC-specific rules.',
          'Separate module paths or workspaces for regulated vs. standard environments.'
        ],
        antiPatterns: 'Style guide exists but agents are not given it as context. Linters run but failures are ignored. Regulated and standard IaC is co-mingled without clear boundaries.'
      },
      {
        practices: [
          'Agents generate IaC using an org-approved module registry.',
          'Plan output is posted to PRs automatically; agent-generated IaC PRs require plan review before approval.',
          'Drift detection runs on a schedule; agents can propose remediation PRs.',
          'Cost estimation (Infracost or similar) runs on agent-generated IaC PRs.'
        ],
        artifacts: [
          'Internal module registry.',
          'CI automation for posting plan output as PR comments.',
          'Drift detection pipeline.',
          'Cost estimation integration.'
        ],
        antiPatterns: 'Agents generate raw resources instead of using approved modules. Plan output is posted but reviewers do not read it. Cost estimates are ignored.'
      },
      {
        practices: [
          'Agents can generate complete, compliant modules from high-level requirements.',
          'Generated modules are automatically validated against OPA/Sentinel/custom policies before plan.',
          'Agent-generated IaC changes to production environments require a separate approval from the infrastructure team.',
          'Regulatory controls (encryption, logging, access restrictions) are enforced by policy-as-code, not by relying on the agent to remember them.'
        ],
        artifacts: [
          'Policy-as-code library (OPA, Sentinel, or equivalent).',
          'Approval workflow for production IaC changes.',
          'Policy-as-code enforcement in CI.'
        ],
        antiPatterns: 'Policy-as-code exists but is not enforced in CI. Agents bypass the module registry for "simple" resources. Production approval is a rubber stamp.'
      },
      {
        practices: [
          'Agents maintain IaC: upgrade provider versions, refactor deprecated resources, update modules to match new compliance requirements.',
          'IaC generation quality is measured: plan accuracy, policy violations caught, rework rate.',
          'Agent context includes live infrastructure state (read-only) to inform generation decisions.'
        ],
        artifacts: [
          'IaC maintenance automation with approval gates.',
          'Quality metrics dashboard for agent-generated IaC.',
          'Read-only infrastructure state access for agents.'
        ],
        antiPatterns: 'Agents have write access to infrastructure state. Automated upgrades are applied without testing. Quality metrics show declining quality but no action is taken.'
      }
    ]
  },
  {
    name: 'Ops and Incident Response',
    description: 'How agents assist during incidents, runbooks, and operational tasks. Safety rules, remediation boundaries, and proactive capabilities.',
    levels: [
      {
        practices: ['None.'],
        artifacts: ['None.'],
        antiPatterns: 'No runbooks exist. Incident response is ad hoc.'
      },
      {
        practices: ['On-call engineers occasionally use agents to search logs, draft incident summaries, or brainstorm root causes.'],
        artifacts: ['None required.'],
        antiPatterns: 'Agents are given production credentials during incidents. Agent suggestions are followed without verification during high-stress incidents.'
      },
      {
        practices: [
          'Agent usage during incidents follows runbook safety rules.',
          'Agents used in ops contexts have read-only access to logs and metrics. No write access to production systems.',
          'Incident summaries generated by agents are reviewed by the incident commander before distribution.'
        ],
        artifacts: [
          'Runbook safety rules for agent usage.',
          'Read-only access role for agent ops access.',
          'Incident summary review checklist.'
        ],
        antiPatterns: 'Agents have the same credentials as the on-call engineer. Agent-generated incident summaries are sent to stakeholders without review.'
      },
      {
        practices: [
          'Agents assist with structured incident tasks: timeline construction, log correlation, similar-incident search, draft postmortems.',
          'Agent actions during incidents are logged to the incident channel and audit trail.',
          'Runbooks include agent-assisted steps with clear handoff points (agent proposes, human executes).'
        ],
        artifacts: [
          'Agent-assisted runbook templates.',
          'Incident audit log including agent actions.',
          'Postmortem template with agent-contribution section.'
        ],
        antiPatterns: 'Agents replace human judgment during incidents. Agent actions are not logged. Postmortems do not capture whether agent assistance helped or hindered.'
      },
      {
        practices: [
          'Agents can execute pre-approved remediation actions (restart a service, scale up capacity, toggle a feature flag) through a controlled interface with approval and audit.',
          'Agent remediation actions are constrained to a pre-approved list; anything outside the list requires human execution.',
          'Incident response metrics track agent contribution: time-to-detection, time-to-mitigation, accuracy of root cause suggestions.'
        ],
        artifacts: [
          'Approved remediation action list with execution interface.',
          'Approval and audit trail for agent-executed remediations.',
          'Incident metrics dashboard with agent contribution data.'
        ],
        antiPatterns: 'Agents execute unapproved actions. The approved action list is never updated. Agent remediation is used as an excuse to reduce on-call staffing.'
      },
      {
        practices: [
          'Agents proactively identify anomalies and propose preventive actions before incidents occur.',
          'Agent remediation accuracy is measured and used to expand or contract the approved action list.',
          'Incident postmortems feed back into agent context and runbooks automatically.'
        ],
        artifacts: [
          'Anomaly detection integration with agent alerting.',
          'Remediation accuracy metrics.',
          'Automated postmortem-to-runbook feedback pipeline.'
        ],
        antiPatterns: 'Proactive alerts create noise without actionable recommendations. Feedback loops exist on paper but are never executed.'
      }
    ]
  },
  {
    name: 'Security and Compliance',
    description: 'How agent usage meets security policies, data classification, and regulatory requirements. DLP, audit, secrets management, and compliance enforcement.',
    levels: [
      {
        practices: ['None.'],
        artifacts: ['None.'],
        antiPatterns: 'No policy on AI agent usage exists. Sensitive data is inadvertently sent to third-party AI services. No awareness of data classification implications.'
      },
      {
        practices: ['Security team is aware that developers use AI agents. Informal guidance exists about not pasting secrets or PII into prompts.'],
        artifacts: ['Informal guidance (chat message, wiki note).'],
        antiPatterns: 'Guidance is informal and inconsistent. No technical controls prevent data leakage to AI endpoints.'
      },
      {
        practices: [
          'Acceptable use policy for AI agents is published.',
          'Data classification rules define what can and cannot be sent to AI agent endpoints (e.g., no PII, no secrets, no classified data).',
          'For regulated environments: AI agent traffic does not leave approved network boundaries.',
          'Secrets scanning runs on all agent-generated code before merge.',
          'Agent API keys are managed centrally, not by individual developers.'
        ],
        artifacts: [
          'AI agent acceptable use policy.',
          'Data classification matrix for AI agent usage.',
          'Regulated environment network boundary documentation.',
          'Secrets scanning configuration.',
          'Centralized API key management.'
        ],
        antiPatterns: 'Policy exists but is not enforced technically. Regulated and standard environments use the same AI endpoints. Developers use personal API keys.'
      },
      {
        practices: [
          'All AI agent API calls are routed through an org-managed proxy or gateway that logs prompts and responses (with PII redaction).',
          'Audit logs capture who used which agent, when, in which repo, and what was sent.',
          'Regular (quarterly) review of agent usage logs for policy violations.',
          'Supply chain security: agent-suggested dependencies are checked against an approved list or scanned for vulnerabilities before merge.'
        ],
        artifacts: [
          'API gateway or proxy with logging.',
          'Audit log pipeline to your monitoring/SIEM system.',
          'Quarterly review process documentation and findings.',
          'Dependency allow-list or vulnerability scanning integration.'
        ],
        antiPatterns: 'Logging captures prompts containing sensitive data without redaction. Audit reviews are performed but findings are not remediated. Dependency scanning exists but agents suggest bypassing it.'
      },
      {
        practices: [
          'DLP (data loss prevention) controls actively prevent sensitive data from reaching AI agent endpoints.',
          'Compliance checks are automated: agent-generated code is scanned for violations of applicable regulatory controls.',
          'Agent access is governed by least-privilege roles scoped to specific repos, actions, and environments.',
          'Exception process exists for cases where agents need access to restricted data or systems, with documented approval and time-bound access.'
        ],
        artifacts: [
          'DLP integration configuration.',
          'Automated compliance scanning pipeline.',
          'Access role definitions for agent access (per-repo or per-team).',
          'Exception request template and approval log.'
        ],
        antiPatterns: 'DLP blocks legitimate agent usage with too many false positives. Compliance scanning produces reports nobody reads. Exception process is so burdensome that teams bypass it.'
      },
      {
        practices: [
          'Security controls for agent usage are continuously tuned based on threat intelligence, audit findings, and false-positive rates.',
          'Agent behavior is monitored for anomalies (unusual volume, unexpected repos, off-hours usage).',
          'The org contributes to industry standards and shares anonymized learnings about AI agent security.'
        ],
        artifacts: [
          'Security control tuning log.',
          'Behavioral anomaly detection for agent usage.',
          'Industry engagement documentation.'
        ],
        antiPatterns: 'Controls are set once and never revisited. Anomaly detection generates alerts that are ignored. The org treats AI agent security as a solved problem.'
      }
    ]
  },
  {
    name: 'Knowledge Management',
    description: 'How agents consume and contribute to org documentation, ADRs, and context. RAG systems, editorial processes, and knowledge gap tracking.',
    levels: [
      {
        practices: ['None.'],
        artifacts: ['None.'],
        antiPatterns: 'Documentation is sparse or outdated. Agents have no useful context to work with.'
      },
      {
        practices: ['Developers occasionally point agents at README files or code comments for context.'],
        artifacts: ['None required.'],
        antiPatterns: 'Agents generate documentation that contradicts existing docs. No single source of truth exists.'
      },
      {
        practices: [
          'Every repo has project-level agent instructions (e.g., CLAUDE.md or equivalent) that describe the project, conventions, architecture, and key decisions.',
          'ADRs (Architecture Decision Records) are maintained and agents are instructed to reference them.',
          'Agents are explicitly told where documentation lives and how to reference it.'
        ],
        artifacts: [
          'Agent instructions per repo (or monorepo section).',
          'ADR directory with numbered records.',
          'Agent context configuration pointing to docs.'
        ],
        antiPatterns: 'Agent instruction files are boilerplate copied between repos. ADRs exist but are never updated. Agents are given too much context (entire repo) instead of curated documents.'
      },
      {
        practices: [
          'Documentation quality is measured (coverage, freshness, developer satisfaction).',
          'Agents assist in maintaining documentation: updating READMEs when code changes, generating API docs, flagging stale docs.',
          'A documentation index exists that agents can search (not just a flat file dump).'
        ],
        artifacts: [
          'Documentation quality metrics.',
          'Agent-assisted doc maintenance workflow.',
          'Searchable documentation index.'
        ],
        antiPatterns: 'Agents generate docs that are never reviewed and contain hallucinated content. Documentation index is out of date.'
      },
      {
        practices: [
          'Agents have access to a curated knowledge base via retrieval-augmented generation (RAG) or similar mechanism.',
          'Knowledge base includes: org standards, approved patterns, past incident learnings, architectural guidelines.',
          'Agent-generated documentation is reviewed and merged into the canonical knowledge base.'
        ],
        artifacts: [
          'RAG pipeline or knowledge retrieval system.',
          'Curated knowledge base with editorial process.',
          'Documentation merge workflow.'
        ],
        antiPatterns: 'RAG retrieves irrelevant or outdated content. Knowledge base has no editorial process and accumulates noise. Agents cite internal docs that were already deprecated.'
      },
      {
        practices: [
          'Knowledge base is continuously refined based on agent usage patterns and developer feedback.',
          'Agents identify knowledge gaps (questions they cannot answer from available context) and flag them for documentation.',
          'Cross-team knowledge sharing is facilitated by agents that can access and synthesize documentation from multiple teams.'
        ],
        artifacts: [
          'Knowledge gap tracking system.',
          'Cross-team knowledge access policies.',
          'Knowledge base refinement changelog.'
        ],
        antiPatterns: 'Knowledge gaps are flagged but never addressed. Cross-team access violates data classification boundaries.'
      }
    ]
  },
  {
    name: 'Governance',
    description: 'How the org manages agent access, approvals, audit, cost tracking, and exceptions. Policy enforcement and cross-functional governance.',
    levels: [
      {
        practices: ['None.'],
        artifacts: ['None.'],
        antiPatterns: 'No one owns AI agent strategy. Adoption is ungoverned.'
      },
      {
        practices: ['At least one person or team is informally tracking AI agent usage and interest across the org.'],
        artifacts: ['Informal tracking (spreadsheet, chat channel).'],
        antiPatterns: 'Multiple teams evaluate agents independently with no coordination. Shadow IT risk.'
      },
      {
        practices: [
          'An AI agent governance body exists (can be a working group, not necessarily a committee).',
          'The governance body maintains the approved tools list, acceptable use policy, and exception process.',
          'Agent usage decisions (new tools, expanded access, policy changes) go through a documented process.',
          'Regulated-environment-specific governance requirements are documented and enforced separately.'
        ],
        artifacts: [
          'Governance body charter or working group terms of reference.',
          'Decision log for agent-related decisions.',
          'Regulated environment governance addendum.'
        ],
        antiPatterns: 'Governance body exists but has no authority. Decisions are made but not communicated. Regulated and standard governance are identical despite different requirements.'
      },
      {
        practices: [
          'Agent usage is reported to leadership regularly (monthly or quarterly).',
          'Cost of agent usage is tracked and allocated to teams or projects.',
          'Risk register includes AI agent-specific risks.',
          'Vendor management process covers AI agent providers.'
        ],
        artifacts: [
          'Leadership report template.',
          'Cost allocation dashboard.',
          'Risk register entries for AI agents.',
          'Vendor assessment for AI agent providers.'
        ],
        antiPatterns: 'Reports are produced but leadership does not read them. Cost tracking is inaccurate. Risk register entries are generic and not actionable.'
      },
      {
        practices: [
          'Governance is embedded in tooling: policy-as-code enforces agent boundaries, approval workflows are automated, audit trails are complete.',
          'Cross-functional governance includes engineering, security, legal, and compliance representatives.',
          'Agent governance aligns with broader IT and data governance frameworks.'
        ],
        artifacts: [
          'Policy-as-code for agent governance.',
          'Cross-functional governance meeting minutes or decision log.',
          'Governance framework alignment documentation.'
        ],
        antiPatterns: 'Automated governance is brittle and blocks legitimate work. Cross-functional meetings happen but produce no decisions. Governance overhead discourages agent adoption.'
      },
      {
        practices: [
          'Governance processes are continuously improved based on measured outcomes (adoption rate, incident rate, compliance findings).',
          'The org anticipates regulatory changes related to AI and proactively adjusts policies.',
          'Governance decisions are transparent and communicated org-wide.'
        ],
        artifacts: [
          'Governance improvement log.',
          'Regulatory horizon scanning process.',
          'Org-wide governance communications.'
        ],
        antiPatterns: 'Governance becomes self-serving and slows innovation without reducing risk. Regulatory scanning is reactive, not proactive.'
      }
    ]
  },
  {
    name: 'Evaluation and Measurement',
    description: 'How the org measures agent value, code quality, ROI, and risk. KPI definitions, cohort comparisons, and decision-driven metrics.',
    levels: [
      {
        practices: ['None.'],
        artifacts: ['None.'],
        antiPatterns: 'No data on agent usage or impact.'
      },
      {
        practices: ['Anecdotal feedback from developers about agent usefulness.'],
        artifacts: ['None required.'],
        antiPatterns: 'Decisions about agent adoption are based on vendor claims or hype, not evidence.'
      },
      {
        practices: [
          'At least 5 KPIs are defined for agent adoption.',
          'Baseline measurements are taken before expanding agent usage.',
          'Developer surveys capture sentiment about agent tools.'
        ],
        artifacts: [
          'KPI definitions document.',
          'Baseline measurement report.',
          'Survey instrument and initial results.'
        ],
        antiPatterns: 'KPIs are defined but not measured. Baselines are not taken, making it impossible to measure improvement. Surveys ask leading questions.'
      },
      {
        practices: [
          'KPIs are measured regularly (monthly minimum).',
          'A/B or cohort comparisons measure agent impact (e.g., teams with agents vs. teams without).',
          'Agent quality is measured: defect rate in agent-generated code, test coverage of agent-generated code, rework rate.'
        ],
        artifacts: [
          'Monthly KPI report.',
          'Cohort comparison analysis.',
          'Agent code quality report.'
        ],
        antiPatterns: 'Metrics show no improvement but agent usage continues to expand. Cohort comparisons are not controlled for team differences. Quality metrics are gamed.'
      },
      {
        practices: [
          'All 12+ KPIs are measured and reviewed.',
          'ROI analysis includes direct costs (API usage, tooling licenses), indirect costs (review overhead, incident rate changes), and benefits (velocity, quality, developer satisfaction).',
          'Measurement drives decisions: tools that do not demonstrate value are deprecated; successful patterns are scaled.'
        ],
        artifacts: [
          'Full KPI dashboard.',
          'ROI analysis report.',
          'Decision log showing measurement-driven actions.'
        ],
        antiPatterns: 'ROI analysis is manipulated to justify predetermined conclusions. Sunk cost fallacy keeps underperforming tools alive. Metrics overhead exceeds the value of measurement.'
      },
      {
        practices: [
          'Measurement framework itself is evaluated and improved.',
          'Leading indicators (not just lagging) are tracked to predict agent impact.',
          'The org shares measurement methodology and anonymized results with the industry.'
        ],
        artifacts: [
          'Measurement framework review log.',
          'Leading indicator definitions and tracking.',
          'Published case studies or anonymized reports.'
        ],
        antiPatterns: 'Measurement becomes an end in itself. The org over-indexes on metrics and loses sight of qualitative developer experience.'
      }
    ]
  }
];

// Render
function init() {
  renderNav();
  renderDomains();
  showDomain(0);

  // Handle hash navigation
  if (window.location.hash) {
    var idx = parseInt(window.location.hash.replace('#domain-', ''), 10);
    if (!isNaN(idx) && idx >= 0 && idx < 9) showDomain(idx);
  }
}

function renderNav() {
  var html = '';
  for (var i = 0; i < DOMAINS.length; i++) {
    html += '<button class="domain-nav-btn" data-idx="' + i + '" onclick="showDomain(' + i + ')">' + (i + 1) + '. ' + DOMAINS[i].name + '</button>';
  }
  document.getElementById('domain-nav').innerHTML = html;
}

function renderDomains() {
  var html = '';
  for (var d = 0; d < DOMAINS.length; d++) {
    var dom = DOMAINS[d];
    html += '<div class="domain-page" id="domain-' + d + '">';

    // Title card
    html += '<div class="domain-title-card" style="border-top-color:' + DOMAIN_COLORS[d] + '">';
    html += '<div class="domain-num">Domain ' + (d + 1) + ' of 9</div>';
    html += '<h2>' + dom.name + '</h2>';
    html += '<p>' + dom.description + '</p>';
    html += '</div>';

    // Level cards
    for (var l = 0; l < 6; l++) {
      var lv = dom.levels[l];
      var collapsed = l > 3 ? ' collapsed' : '';
      html += '<div class="level-card' + collapsed + '" style="border-left-color:' + LEVEL_COLORS[l] + '" id="lc-' + d + '-' + l + '">';

      html += '<div class="level-card-header" onclick="toggleLevel(\'lc-' + d + '-' + l + '\')">';
      html += '<div class="level-badge" style="background:' + LEVEL_COLORS[l] + '">' + l + '</div>';
      html += '<h3>Level ' + l + ' -- ' + LEVEL_NAMES[l] + '</h3>';
      html += '<span class="level-toggle">&#9660;</span>';
      html += '</div>';

      html += '<div class="level-card-body">';

      // Required practices
      html += '<div class="detail-section"><h4>Required Practices</h4><ul>';
      for (var p = 0; p < lv.practices.length; p++) {
        html += '<li>' + lv.practices[p] + '</li>';
      }
      html += '</ul></div>';

      // Required artifacts
      html += '<div class="detail-section"><h4>Required Artifacts</h4>';
      if (lv.artifacts.length === 1 && (lv.artifacts[0] === 'None.' || lv.artifacts[0] === 'None required.' || lv.artifacts[0].indexOf('None') === 0)) {
        html += '<p style="font-size:0.88rem;color:var(--color-text-muted)">' + lv.artifacts[0] + '</p>';
      } else {
        html += '<ul>';
        for (var a = 0; a < lv.artifacts.length; a++) {
          html += '<li>' + lv.artifacts[a] + '</li>';
        }
        html += '</ul>';
      }
      html += '</div>';

      // Anti-patterns
      html += '<div class="detail-section"><h4>Anti-Patterns to Watch For</h4>';
      html += '<div class="anti-pattern-box">' + lv.antiPatterns + '</div>';
      html += '</div>';

      html += '</div>'; // level-card-body
      html += '</div>'; // level-card
    }

    // Prev/next nav
    html += '<div class="page-nav">';
    if (d > 0) {
      html += '<button class="btn btn-secondary" onclick="showDomain(' + (d - 1) + ')">&larr; ' + DOMAINS[d - 1].name + '</button>';
    } else {
      html += '<span></span>';
    }
    if (d < 8) {
      html += '<button class="btn btn-primary" onclick="showDomain(' + (d + 1) + ')">' + DOMAINS[d + 1].name + ' &rarr;</button>';
    } else {
      html += '<span></span>';
    }
    html += '</div>';

    html += '</div>'; // domain-page
  }
  document.getElementById('domains-container').innerHTML = html;
}

function showDomain(idx) {
  document.querySelectorAll('.domain-page').forEach(function(p) { p.classList.remove('active'); });
  document.getElementById('domain-' + idx).classList.add('active');
  document.querySelectorAll('.domain-nav-btn').forEach(function(b) { b.classList.remove('active'); });
  document.querySelector('.domain-nav-btn[data-idx="' + idx + '"]').classList.add('active');
  window.location.hash = 'domain-' + idx;
  window.scrollTo(0, 0);
}

function toggleLevel(id) {
  document.getElementById(id).classList.toggle('collapsed');
}

init();
</script>
</body>
</html>
